{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import books - PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "import logging\n",
    "\n",
    "# sset logging level to INFO\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder with the books to be imported\n",
    "path_books = './books/import'\n",
    "# Path to the folder with the books that have already been imported\n",
    "path_done_books = './books/done'\n",
    "# Path to the folder with vector store\n",
    "path_vector_store = '../vector-store/chroma/books'\n",
    "# Chunk size for the vector store\n",
    "# chunkSize controls the max size (in terms of number of characters) of the final documents. \n",
    "# chunkOverlap specifies how much overlap there should be between chunks. \n",
    "# This is often helpful to make sure that the text isn't split weirdly.\n",
    "chunk_size = 500\n",
    "# Define embedding model\n",
    "embeddings = OllamaEmbeddings(model='nomic-embed-text:v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_books(path_books):\n",
    "    \"\"\"\n",
    "    Load books from the specified path.\n",
    "\n",
    "    Args:\n",
    "        path_books (str): The path to the directory containing the books.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of file paths for the books found in the directory.\n",
    "    \"\"\"\n",
    "    logging.info(f'Loading books from {path_books}')\n",
    "    books = []\n",
    "    for root, dirs, files in os.walk(path_books):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                books.append(os.path.join(root, file))\n",
    "    return books\n",
    "\n",
    "def load_pdf(file):\n",
    "    \"\"\"\n",
    "    Loads a PDF file and returns the extracted documents with PyPDFLoader.\n",
    "\n",
    "    Args:\n",
    "        file (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted documents from the PDF.\n",
    "\n",
    "    \"\"\"\n",
    "    logging.info(f'Loading PDF {file}')\n",
    "    loader = PyPDFLoader(file)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def load_chunked_docs(documents):\n",
    "    \"\"\"\n",
    "    Chunk documents.\n",
    "\n",
    "    Args:\n",
    "        documents (list): A list of documents to be chunked.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of chunked documents.\n",
    "\n",
    "    \"\"\"\n",
    "    logging.info(f'Chunking documents')\n",
    "    text_spliter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "    chunked_docs = text_spliter.split_documents(documents)\n",
    "    return chunked_docs\n",
    "\n",
    "def load_vector_store(chunked_docs, path_vector_store):\n",
    "    \"\"\"\n",
    "    Loads or creates a vector store using FAISS.\n",
    "\n",
    "    Args:\n",
    "        chunked_docs (list): A list of chunked documents.\n",
    "        path_vector_store (str): The path to the vector store.\n",
    "\n",
    "    Returns:\n",
    "        FAISS object: The loaded or created vector store.\n",
    "\n",
    "    Raises:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(path_vector_store):\n",
    "        logging.info(f'Creating vector store')\n",
    "        os.makedirs(path_vector_store)\n",
    "        db = Chroma.from_documents(chunked_docs, embeddings, persist_directory=path_vector_store)\n",
    "        return db\n",
    "    else:\n",
    "        logging.info(f'Loading vector store')\n",
    "        db = Chroma(persist_directory=path_vector_store, embedding_function=embeddings)\n",
    "        logging.info(f'Adding documents to vector store')\n",
    "        db.add_documents(chunked_docs)\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading books from ./books/import\n",
      "INFO:root:Processing book ./books/import/Por que tarda o pleno Avivamento - Leonard Ravenhill.pdf\n",
      "INFO:root:Loading PDF ./books/import/Por que tarda o pleno Avivamento - Leonard Ravenhill.pdf\n",
      "INFO:root:Chunking documents\n",
      "INFO:root:Creating vector store\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:root:Moving book ./books/import/Por que tarda o pleno Avivamento - Leonard Ravenhill.pdf to ./books/done\n",
      "INFO:root:Processing book ./books/import/Sociedade do Cansaço - Byung-Chul Han.pdf\n",
      "INFO:root:Loading PDF ./books/import/Sociedade do Cansaço - Byung-Chul Han.pdf\n",
      "INFO:root:Chunking documents\n",
      "INFO:root:Loading vector store\n",
      "INFO:root:Adding documents to vector store\n",
      "INFO:root:Moving book ./books/import/Sociedade do Cansaço - Byung-Chul Han.pdf to ./books/done\n",
      "INFO:root:Processing book ./books/import/Somos Todos Teólogos - RC Sproul.pdf\n",
      "INFO:root:Loading PDF ./books/import/Somos Todos Teólogos - RC Sproul.pdf\n",
      "INFO:root:Chunking documents\n",
      "INFO:root:Loading vector store\n",
      "INFO:root:Adding documents to vector store\n",
      "INFO:root:Moving book ./books/import/Somos Todos Teólogos - RC Sproul.pdf to ./books/done\n",
      "INFO:root:Processing book ./books/import/Não coisas - Byung Chul Han.pdf\n",
      "INFO:root:Loading PDF ./books/import/Não coisas - Byung Chul Han.pdf\n",
      "INFO:root:Chunking documents\n",
      "INFO:root:Loading vector store\n",
      "INFO:root:Adding documents to vector store\n",
      "INFO:root:Moving book ./books/import/Não coisas - Byung Chul Han.pdf to ./books/done\n",
      "INFO:root:Processing book ./books/import/Conselhos para obreiros - Charles H Spurgeon.pdf\n",
      "INFO:root:Loading PDF ./books/import/Conselhos para obreiros - Charles H Spurgeon.pdf\n",
      "INFO:root:Chunking documents\n",
      "INFO:root:Loading vector store\n",
      "INFO:root:Adding documents to vector store\n",
      "INFO:root:Moving book ./books/import/Conselhos para obreiros - Charles H Spurgeon.pdf to ./books/done\n",
      "INFO:root:Processing book ./books/import/O Peregrino - John Bunyan.pdf\n",
      "INFO:root:Loading PDF ./books/import/O Peregrino - John Bunyan.pdf\n",
      "INFO:root:Chunking documents\n",
      "INFO:root:Loading vector store\n",
      "INFO:root:Adding documents to vector store\n",
      "INFO:root:Moving book ./books/import/O Peregrino - John Bunyan.pdf to ./books/done\n",
      "INFO:root:Processing book ./books/import/Evangelho Maltrapilho - Brennan Manning.pdf\n",
      "INFO:root:Loading PDF ./books/import/Evangelho Maltrapilho - Brennan Manning.pdf\n",
      "INFO:root:Chunking documents\n",
      "INFO:root:Loading vector store\n",
      "INFO:root:Adding documents to vector store\n",
      "INFO:root:Moving book ./books/import/Evangelho Maltrapilho - Brennan Manning.pdf to ./books/done\n"
     ]
    }
   ],
   "source": [
    "# 10min\n",
    "# Load books from path\n",
    "books = load_books(path_books)\n",
    "# Process each book\n",
    "for book in books:\n",
    "    logging.info(f'Processing book {book}')\n",
    "    documents_books = load_pdf(book)\n",
    "    chunked_docs = load_chunked_docs(documents_books)\n",
    "    db = load_vector_store(chunked_docs=chunked_docs, path_vector_store=path_vector_store)\n",
    "    logging.info(f'Moving book {book} to {path_done_books}')\n",
    "    os.rename(book, os.path.join(path_done_books, os.path.basename(book)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cristian/miniconda3/envs/crewai/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#testar se o documento foi adicionado\n",
    "#db = FAISS.load_local(\"../vector-store/books\", embeddings, allow_dangerous_deserialization=True)\n",
    "db = Chroma(persist_directory=path_vector_store, embedding_function=embeddings)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    ")\n",
    "\n",
    "results = retriever.get_relevant_documents(\"Calvinismo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Page Content: \n",
      "5\n",
      " John Calvin, \n",
      "Commentaries on the Epistle of Paul to the Romans\n",
      ", trans. and ed. John Owen (repr., Grand Rapids: Baker, 2003),\n",
      "354.\n",
      "\n",
      "Source: \n",
      "./books/import/Somos Todos Teólogos - RC Sproul.pdf \n",
      "\n",
      "Page:\n",
      "94\n",
      "\n",
      "\n",
      "\n",
      "Page Content: \n",
      "de nosso coração? Será que poderíamos convidar o Esp írito Santo \n",
      "para caminhar conosco de mãos dadas, p elos corredores del e? Não\n",
      "\n",
      "Source: \n",
      "./books/import/Por que tarda o pleno Avivamento - Leonard Ravenhill.pdf \n",
      "\n",
      "Page:\n",
      "21\n",
      "\n",
      "\n",
      "\n",
      "Page Content: \n",
      "92 HENRI J. M. NOUWEN . ʺTHE PRODIGAL  COMES HOMEʺ, National Catholic Reporter,  4 DE AGOSTO  DE 1989. \n",
      "93 Robert BOLT. A man for all seasons. Nova York: Random  House, 1960, p. 140. \n",
      "94 G. K. CHESTERTON.  The fame of blessed Thomas More. Nova York: Sheed and Ward, 1929, p.6V\n",
      "\n",
      "Source: \n",
      "./books/import/Evangelho Maltrapilho - Brennan Manning.pdf \n",
      "\n",
      "Page:\n",
      "88\n",
      "\n",
      "\n",
      "\n",
      "Page Content: \n",
      "7\n",
      " Repare semelhança com 2Coríntios 11:13-14.\n",
      "\n",
      "Source: \n",
      "./books/import/O Peregrino - John Bunyan.pdf \n",
      "\n",
      "Page:\n",
      "197\n",
      "\n",
      "\n",
      "\n",
      "Page Content: \n",
      "numa sala, chances estatísticas podem ser usadas para mostrar a possibilidade\n",
      "de certo número de moscas estarem numa determinada polegada quadrada da\n",
      "\n",
      "Source: \n",
      "./books/import/Somos Todos Teólogos - RC Sproul.pdf \n",
      "\n",
      "Page:\n",
      "107\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in results:\n",
    "    page_content = row.page_content\n",
    "    source = row.metadata['source']\n",
    "    page = row.metadata['page']\n",
    "\n",
    "    \n",
    "\n",
    "    # Agora você pode fazer o que quiser com esses dados.\n",
    "    # Por exemplo, você pode imprimi-los:\n",
    "    print(f\"\"\"\n",
    "Page Content: \n",
    "{page_content.replace('\t',' ')}\n",
    "\n",
    "Source: \n",
    "{source} \n",
    "\n",
    "Page:\n",
    "{page}\n",
    "\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
